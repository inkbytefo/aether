model:
  d_model: 768
  n_layer: 24
  vocab_size: 50257
  ssm_cfg:
    d_state: 32
    expand: 2
    dt_rank: "auto"
    d_conv: 4
    dt_min: 0.001
    dt_max: 0.1
    dt_init: "random"
    dt_scale: 1.0
    dt_init_floor: 1e-4
    conv_bias: true
    bias: false
    use_fast_path: true
  rms_norm: true
  residual_in_fp32: true
  fused_add_norm: true
  pad_vocab_size_multiple: 8

training:
  batch_size: 32
  sequence_len: 2048
  learning_rate: 6.0e-4
  min_lr: 6.0e-5
  warmup_steps: 1000
  weight_decay: 0.1
  gradient_accumulation_steps: 1
  max_grad_norm: 1.0
  seed: 42
  
data:
  tokenizer_path: "data/tokenizer.model" # Placeholder, will be updated after training
  train_split: 0.9
  num_workers: 4
