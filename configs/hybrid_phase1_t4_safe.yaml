## Developer: inkbytefo
## Modified: 2025-11-22

# Hybrid Mamba-Attention Configuration - T4 SAFE MODE (Conservative)
# Use this if you get CUDA OOM errors with hybrid_phase1.yaml
# Estimated Model Size: ~150M parameters
# Peak VRAM Usage: ~8GB (with FP16 mixed precision)

model:
  vocab_size: 50257
  d_model: 512         # Reduced from 768
  n_layer: 12          # Reduced from 24 (8 Mamba + 4 Attention)
  d_state: 16
  d_conv: 4
  expand: 2
  use_plasticity: false
  
  # Hybrid-specific
  attention_interval: 3  # Every 3rd layer is Linear Attention

data:
  train_path: "data/corpus_v1/train.bin"
  val_path: "data/corpus_v1/val.bin"
  tokenizer_path: "data/corpus_v1/tokenizer.model"
  seq_length: 512
  batch_size: 16       # Can use higher batch with smaller model

training:
  max_steps: 150000    # More steps needed for smaller model
  learning_rate: 5.0e-4
  weight_decay: 0.1
  warmup_steps: 2000
  gradient_accumulation_steps: 4  # Effective batch = 16 * 4 = 64
  max_grad_norm: 1.0
  device: "cuda"
  
  # Optimizer
  optimizer: "AdamW"
  beta1: 0.9
  beta2: 0.95
  
  # Scheduler
  scheduler: "cosine"
  min_lr: 1.0e-5

logging:
  log_interval: 10
  val_interval: 500
  save_interval: 2000
