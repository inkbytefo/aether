model:
  d_model: 256
  n_layer: 6
  vocab_size: 50257  # Updated to match SentencePiece tokenizer
  ssm_cfg:
    d_state: 16
    d_conv: 4
    expand: 2
  use_plasticity: false # Disabled for Phase 1

training:
  batch_size: 8  # Reduced for Tesla T4 (15GB VRAM)
  learning_rate: 0.0005
  max_steps: 10000
  seed: 42
  device: "cuda"
  gradient_accumulation_steps: 16  # Effective batch = 8 * 16 = 128
  max_grad_norm: 1.0

data:
  dataset_name: "phase1_tr"
  max_length: 512
  tokenizer_path: "data/corpus_v1/tokenizer.model"  # Added tokenizer path
  dataset_paths:
    - "data/corpus_v1/train.bin"  # Updated to match prepare_phase1_tr.py output
    - "data/corpus_v1/val.bin"
