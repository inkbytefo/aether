model:
  d_model: 256
  n_layer: 4
  vocab_size: 50257
  ssm_cfg:
    d_state: 16
    d_conv: 4
    expand: 2
  use_plasticity: true # Flag to trigger PlasticMambaLLM

training:
  batch_size: 8
  learning_rate: 0.0001 # Even lower LR for plasticity training
  max_steps: 2000
  seed: 42
  device: "cuda"

data:
  dataset_name: "mixed"
  max_length: 512
  dataset_paths:
    - "data/processed/code"
    - "data/processed/math"
