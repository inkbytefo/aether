model:
  d_model: 256
  n_layer: 4
  vocab_size: 50257 # GPT-2 tokenizer default
  ssm_cfg:
    d_state: 16
    d_conv: 4
    expand: 2

training:
  batch_size: 32
  learning_rate: 0.001
  max_steps: 1000
  seed: 42
  device: "cuda" # or "cpu"

data:
  dataset_name: "roneneldan/TinyStories"
  max_length: 512
