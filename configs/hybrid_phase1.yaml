## Developer: inkbytefo
## Modified: 2025-11-22

# Hybrid Mamba-Attention Configuration - T4 GPU Optimized (16GB VRAM)
# Architecture: Jamba-style (Mamba + Linear Attention)
# Estimated Model Size: ~500M parameters
# Peak VRAM Usage: ~12GB (with FP16 mixed precision)

model:
  vocab_size: 50257
  d_model: 768         # Hidden dimension
  n_layer: 24          # Total layers (16 Mamba + 8 Attention)
  d_state: 16          # SSM state dimension
  d_conv: 4            # Convolution kernel size
  expand: 2            # Expansion factor for MLP
  use_plasticity: false
  
  # Hybrid-specific
  attention_interval: 3  # Every 3rd layer is Linear Attention

data:
  train_path: "data/corpus_v1/train.bin"
  val_path: "data/corpus_v1/val.bin"
  tokenizer_path: "data/corpus_v1/tokenizer.model"
  seq_length: 512      # T4 optimized (was 1024)
  batch_size: 10       # T4 optimized (was 8)

training:
  max_steps: 100000    # Increased for better convergence
  learning_rate: 4.0e-4
  weight_decay: 0.1
  warmup_steps: 2000
  gradient_accumulation_steps: 8  # Effective batch = 10 * 8 = 80
  max_grad_norm: 1.0
  device: "cuda"
  
  # Optimizer
  optimizer: "AdamW"
  beta1: 0.9
  beta2: 0.95
  
  # Scheduler
  scheduler: "cosine"
  min_lr: 5.0e-6

logging:
  log_interval: 10
  val_interval: 500
  save_interval: 2000
