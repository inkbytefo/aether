## Developer: inkbytefo
## Modified: 2025-11-22

# Hybrid Mamba-Attention Configuration (Phase 1 - Production)
# Architecture: Jamba-style (Mamba + Linear Attention)

model:
  vocab_size: 50257
  d_model: 768
  n_layer: 24
  d_state: 16
  d_conv: 4
  expand: 2
  use_plasticity: false  # Not needed - using Linear Attention instead
  
  # Hybrid-specific
  attention_interval: 3  # Every 3rd layer is Linear Attention

data:
  train_path: "data/corpus_v1/train.bin"
  val_path: "data/corpus_v1/val.bin"
  tokenizer_path: "data/corpus_v1/tokenizer.model"
  seq_length: 1024
  batch_size: 8

training:
  max_steps: 50000
  learning_rate: 3.0e-4
  weight_decay: 0.1
  warmup_steps: 2000
  gradient_accumulation_steps: 4
  max_grad_norm: 1.0
  device: "cuda"
  
  # Optimizer
  optimizer: "AdamW"
  beta1: 0.9
  beta2: 0.95
  
  # Scheduler
  scheduler: "cosine"
  min_lr: 3.0e-5

logging:
  log_interval: 10
  val_interval: 500
  save_interval: 2000
